{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.patches import Ellipse\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.spatial import Delaunay\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "import json\n",
    "import networkx as nx\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_distance_gaussian(mean1, cov1, mean2, cov2):\n",
    "    # Compute the squared difference between the means.\n",
    "    mean_diff_sq = np.linalg.norm(mean1 - mean2)**2\n",
    "\n",
    "    # Compute the matrix square root of cov1\n",
    "    cov1_sqrt = sqrtm(cov1)\n",
    "    \n",
    "    # Compute the product cov1_sqrt * cov2 * cov1_sqrt\n",
    "    product = cov1_sqrt @ cov2 @ cov1_sqrt\n",
    "    \n",
    "    # Compute the square root of the product matrix\n",
    "    sqrt_product = sqrtm(product)\n",
    "    \n",
    "    # Compute the trace term\n",
    "    trace_term = np.trace(cov1 + cov2 - 2 * sqrt_product)\n",
    "    \n",
    "    # Return the 2-Wasserstein distance (ensure non-negative)\n",
    "    return np.sqrt(max(mean_diff_sq + trace_term, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emd(formation1, formation2):\n",
    "    n = len(formation1)\n",
    "    m = len(formation2)\n",
    "    cost_matrix = np.zeros((n, m))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            mean1, cov1 = formation1[i]\n",
    "            mean2, cov2 = formation2[j]\n",
    "            cost_matrix[i, j] = wasserstein_distance_gaussian(mean1, cov1, mean2, cov2)\n",
    "    \n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    total_cost = cost_matrix[row_ind, col_ind].sum()\n",
    "    \n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tracks.json', 'r') as f:\n",
    "#     tracks_z14 = json.load(f)\n",
    "\n",
    "with open('tracks.json', 'r') as f:\n",
    "    tracks_z14 = json.load(f)\n",
    "\n",
    "with open('extern_adjusted_tracks_mirrored_5min_window.json', 'r') as f:\n",
    "    tracks_extern = json.load(f)\n",
    "\n",
    "with open('adjusted_tracks_mirroed_5-10min_window_2.json', 'r') as f:\n",
    "    tracks_extern_2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two JSON structures\n",
    "combined_data = {**tracks_z14, **tracks_extern, **tracks_extern_2}\n",
    "\n",
    "with open(\"merged.json\", \"w\") as output_file:\n",
    "    json.dump(combined_data, output_file, indent=4)\n",
    "\n",
    "print(\"Merged JSON saved as 'merged.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define colors for each role (0 to 9)\n",
    "colors = ['green', 'red', 'blue', 'purple', 'orange', 'brown', 'pink', 'cyan', 'magenta', 'yellow']\n",
    "\n",
    "# Create plots for each match entry\n",
    "for file_path, file_data in combined_data.items():\n",
    "    # Create a figure with 2 subplots for roles_0 and roles_1\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Plot for {file_path}\")\n",
    "\n",
    "    # Plot for roles_0\n",
    "    # Plot for roles_0\n",
    "    if \"roles_0\" in file_data:  # Check if roles_0 exists\n",
    "        for key in range(10):  # We have 10 keys (0-9)\n",
    "            if str(key) in file_data[\"roles_0\"]:\n",
    "                mean = file_data[\"roles_0\"][str(key)][\"mean\"]\n",
    "                ax0.scatter(mean[0], mean[1], color=colors[key], label=f'Key {key}')\n",
    "        ax0.set_title(\"roles_0 Mean Values\")\n",
    "        ax0.set_xlabel(\"Mean X\")\n",
    "        ax0.set_ylabel(\"Mean Y\")\n",
    "        ax0.grid(True)\n",
    "        ax0.legend(title=\"Keys\")\n",
    "    else:\n",
    "        ax0.set_title(\"roles_0 Not Available\")\n",
    "        ax0.axis('off')\n",
    "\n",
    "    # Plot for roles_1\n",
    "    if \"roles_1\" in file_data:  # Check if roles_1 exists\n",
    "        for key in range(10):  # We have 10 keys (0-9)\n",
    "            if str(key) in file_data[\"roles_1\"]:\n",
    "                mean = file_data[\"roles_1\"][str(key)][\"mean\"]\n",
    "                ax1.scatter(mean[0], mean[1], color=colors[key], label=f'Key {key}')\n",
    "        ax1.set_title(\"roles_1 Mean Values\")\n",
    "        ax1.set_xlabel(\"Mean X\")\n",
    "        ax1.set_ylabel(\"Mean Y\")\n",
    "        ax1.grid(True)\n",
    "        ax1.legend(title=\"Keys\")\n",
    "    else:\n",
    "        ax1.set_title(\"roles_1 Not Available\")\n",
    "        ax1.axis('off')\n",
    "\n",
    "    # Adjust layout for better visualization\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust so that suptitle doesn't overlap with plots\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formations = []  \n",
    "team_names = []  \n",
    "\n",
    "for team, roles in combined_data.items():\n",
    "    for role_key, players in roles.items():\n",
    "        if not players:  # Skip if the role is an empty object\n",
    "            continue\n",
    "        formation = []\n",
    "        for player, stats in players.items():\n",
    "            mean = np.array(stats['mean'])          \n",
    "            covariance = np.array(stats['covariance']) \n",
    "            formation.append((mean, covariance))\n",
    "        formations.append(formation)\n",
    "        team_names.append(f\"{team}_{role_key}\")\n",
    "\n",
    "num_formations = len(formations)\n",
    "print(f\"Number of formations: {num_formations}\")\n",
    "distance_matrix = np.zeros((num_formations, num_formations))\n",
    "\n",
    "for i in range(num_formations):\n",
    "    for j in range(i + 1, num_formations):\n",
    "        distance = compute_emd(formations[i], formations[j])\n",
    "        distance_matrix[i, j] = distance\n",
    "        distance_matrix[j, i] = distance  # distance is symmetric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(distance_matrix, cmap='viridis', xticklabels=team_names, yticklabels=[name[:15] + '...' if len(name) > 15 else name for name in team_names], square=True)\n",
    "plt.title(\"Distance Matrix Heatmap\")\n",
    "plt.xlabel(\"Teams/Formations\")\n",
    "plt.ylabel(\"Teams/Formations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Truncate labels to a maximum of 15 characters\n",
    "short_labels = [label[:25] + '...' if len(label) > 15 else label for label in team_names]\n",
    "\n",
    "condensed_distance = squareform(distance_matrix)\n",
    "linkage_matrix = linkage(condensed_distance, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(14, 10))  \n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=short_labels,  # Use shortened labels\n",
    "    leaf_rotation=90,  \n",
    "    leaf_font_size=10  \n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram', fontsize=16)\n",
    "plt.xlabel('Team / Formation', fontsize=12)\n",
    "plt.ylabel('EMD Distance', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering = AgglomerativeClustering(\n",
    "    n_clusters=4,\n",
    "    metric='euclidean',  \n",
    "    linkage='ward' \n",
    ")\n",
    "\n",
    "cluster_labels = clustering.fit_predict(distance_matrix)\n",
    "\n",
    "for team, label in zip(team_names, cluster_labels):\n",
    "    print(f\"Team {team} is in cluster {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn means and covs to feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"merged.json\", \"r\") as file:  \n",
    "    data = json.load(file)\n",
    "\n",
    "team_feature_vectors = {}\n",
    "\n",
    "for dataset_key, teams in data.items():\n",
    "    team_feature_vectors[dataset_key] = {}  \n",
    "\n",
    "    \n",
    "    for team_key, players in teams.items():\n",
    "        team_vector = []  \n",
    "\n",
    "        # exactly 10 players in order\n",
    "        player_keys = sorted(players.keys(), key=int)[:10] \n",
    "\n",
    "        for player_key in player_keys:\n",
    "            player_data = players[player_key]\n",
    "            mean = player_data[\"mean\"]\n",
    "            cov_matrix = player_data[\"covariance\"]\n",
    "            cov_vector = [cov_matrix[0][0], cov_matrix[0][1], cov_matrix[1][1]]\n",
    "\n",
    "            player_vector = mean + cov_vector\n",
    "\n",
    "            team_vector.extend(player_vector)\n",
    "\n",
    "        # Store team vector\n",
    "        team_feature_vectors[dataset_key][team_key] = team_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "with open(\"merged.json\", \"r\") as file: \n",
    "    data = json.load(file)\n",
    "\n",
    "ground_truth = {  \n",
    "    (\"./xy_data/alt2_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/alt2_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/alt_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/alt_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/fis1_data.txt\", \"roles_0\"): 442,\n",
    "    (\"./xy_data/fis1_data.txt\", \"roles_1\"): 4231,\n",
    "\n",
    "    (\"./xy_data/fis2_data.txt\", \"roles_0\"): 442,\n",
    "    (\"./xy_data/fis2_data.txt\", \"roles_1\"): 4231,\n",
    "\n",
    "    (\"./xy_data/fried1_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/friedr2_data.txt\", \"roles_0\"): 442,\n",
    "    (\"./xy_data/friedr2_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/gr1_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/gr1_data.txt\", \"roles_1\"): 352,\n",
    "\n",
    "    (\"./xy_data/gr2_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/gr2_data.txt\", \"roles_1\"): 352,\n",
    "\n",
    "    (\"./xy_data/m1_data.txt\", \"roles_0\"): 442,\n",
    "    (\"./xy_data/m1_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/m2_data.txt\", \"roles_0\"): 442,\n",
    "    (\"./xy_data/m2_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/st1_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/st1_data.txt\", \"roles_1\"): 4231,\n",
    "\n",
    "    (\"./xy_data/st2_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/st2_data.txt\", \"roles_1\"): 4231,\n",
    "\n",
    "    (\"./xy_data/stv1_data.txt\", \"roles_0\"): 4231,\n",
    "\n",
    "    (\"./xy_data/stv2_data.txt\", \"roles_0\"): 352,\n",
    "    (\"./xy_data/stv2_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/t1_data.txt\", \"roles_0\"): 0,\n",
    "    (\"./xy_data/t1_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/t2_data.txt\", \"roles_0\"): 0,\n",
    "    (\"./xy_data/t2_data.txt\", \"roles_0\"): 442,\n",
    "\n",
    "    \n",
    "   \n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WMX.xml', 'DFL-CLU-00000G'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WMX.xml', 'DFL-CLU-000008'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WN1.xml', 'DFL-CLU-00000B'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WN1.xml', 'DFL-CLU-00000S'): 0, #here 4332\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WQQ.xml', 'DFL-CLU-00000P'): 352,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WQQ.xml', 'DFL-CLU-00000H'): 352,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WR9.xml', 'DFL-CLU-00000I'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WR9.xml', 'DFL-CLU-00000P'): 442, #here 4422\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WPY.xml', 'DFL-CLU-000005'): 0, #here 4132\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WPY.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOH.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOH.xml', 'DFL-CLU-000011'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOY.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOY.xml', 'DFL-CLU-00000Q'): 352,\n",
    "    #-------------------------\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WMX_2.xml', 'DFL-CLU-00000G'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WMX_2.xml', 'DFL-CLU-000008'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WN1_2.xml', 'DFL-CLU-00000B'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WN1_2.xml', 'DFL-CLU-00000S'): 0, # Changed 4231 to 4332 here\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WQQ_2.xml', 'DFL-CLU-00000P'): 352,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WQQ_2.xml', 'DFL-CLU-00000H'): 352,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WR9_2.xml', 'DFL-CLU-00000I'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WR9_2.xml', 'DFL-CLU-00000P'): 442, # Changed 442 to 4422 here\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WPY_2.xml', 'DFL-CLU-000005'): 0, # Changed 442 to 4132 here\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WPY_2.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOH_2.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOH_2.xml', 'DFL-CLU-000011'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOY_2.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOY_2.xml', 'DFL-CLU-00000Q'): 352\n",
    "}\n",
    "\n",
    "ground_truth_balanced = {  \n",
    "    # (\"./xy_data/alt2_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/alt2_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    # (\"./xy_data/alt_data.txt\", \"roles_0\"): 4231,\n",
    "    # (\"./xy_data/alt_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    # (\"./xy_data/fis1_data.txt\", \"roles_0\"): 442,\n",
    "    # (\"./xy_data/fis1_data.txt\", \"roles_1\"): 4231,\n",
    "\n",
    "    # (\"./xy_data/fis2_data.txt\", \"roles_0\"): 442,\n",
    "    (\"./xy_data/fis2_data.txt\", \"roles_1\"): 4231,\n",
    "\n",
    "    (\"./xy_data/fried1_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    # (\"./xy_data/friedr2_data.txt\", \"roles_0\"): 442,\n",
    "    (\"./xy_data/friedr2_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    # (\"./xy_data/gr1_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/gr1_data.txt\", \"roles_1\"): 352,\n",
    "\n",
    "    # (\"./xy_data/gr2_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/gr2_data.txt\", \"roles_1\"): 352,\n",
    "\n",
    "    (\"./xy_data/m1_data.txt\", \"roles_0\"): 442,\n",
    "    # (\"./xy_data/m1_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    # (\"./xy_data/m2_data.txt\", \"roles_0\"): 442,\n",
    "    (\"./xy_data/m2_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    # (\"./xy_data/st1_data.txt\", \"roles_0\"): 4231,\n",
    "    # (\"./xy_data/st1_data.txt\", \"roles_1\"): 4231,\n",
    "\n",
    "    (\"./xy_data/st2_data.txt\", \"roles_0\"): 4231,\n",
    "    (\"./xy_data/st2_data.txt\", \"roles_1\"): 4231,\n",
    "\n",
    "    (\"./xy_data/stv1_data.txt\", \"roles_0\"): 4231,\n",
    "\n",
    "    (\"./xy_data/stv2_data.txt\", \"roles_0\"): 352,\n",
    "    (\"./xy_data/stv2_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/t1_data.txt\", \"roles_0\"): 0,\n",
    "    (\"./xy_data/t1_data.txt\", \"roles_1\"): 442,\n",
    "\n",
    "    (\"./xy_data/t2_data.txt\", \"roles_0\"): 0,\n",
    "    (\"./xy_data/t2_data.txt\", \"roles_0\"): 442,\n",
    "\n",
    "    \n",
    "   \n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WMX.xml', 'DFL-CLU-00000G'): 4231,\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WMX.xml', 'DFL-CLU-000008'): 4231,\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WN1.xml', 'DFL-CLU-00000B'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WN1.xml', 'DFL-CLU-00000S'): 0, #here 4332\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WQQ.xml', 'DFL-CLU-00000P'): 352,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WQQ.xml', 'DFL-CLU-00000H'): 352,\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WR9.xml', 'DFL-CLU-00000I'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WR9.xml', 'DFL-CLU-00000P'): 442, #here 4422\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WPY.xml', 'DFL-CLU-000005'): 0, #here 4132\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WPY.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOH.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOH.xml', 'DFL-CLU-000011'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOY.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOY.xml', 'DFL-CLU-00000Q'): 352,\n",
    "    #-------------------------\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WMX_2.xml', 'DFL-CLU-00000G'): 4231,\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WMX_2.xml', 'DFL-CLU-000008'): 4231,\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WN1_2.xml', 'DFL-CLU-00000B'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000001_DFL-MAT-J03WN1_2.xml', 'DFL-CLU-00000S'): 0, # Changed 4231 to 4332 here\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WQQ_2.xml', 'DFL-CLU-00000P'): 352,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WQQ_2.xml', 'DFL-CLU-00000H'): 352,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WR9_2.xml', 'DFL-CLU-00000I'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WR9_2.xml', 'DFL-CLU-00000P'): 442, # Changed 442 to 4422 here\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WPY_2.xml', 'DFL-CLU-000005'): 0, # Changed 442 to 4132 here\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WPY_2.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOH_2.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOH_2.xml', 'DFL-CLU-000011'): 4231,\n",
    "    # ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOY_2.xml', 'DFL-CLU-00000P'): 4231,\n",
    "    ('./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WOY_2.xml', 'DFL-CLU-00000Q'): 352\n",
    "}\n",
    "# MEANS!!\n",
    "# X, y = [], []\n",
    "# team_feature_vectors = {}\n",
    "# for dataset_key, teams in data.items():\n",
    "#     team_feature_vectors[dataset_key] = {}\n",
    "#     for team_key, players in teams.items():\n",
    "#         means = []\n",
    "#         covariances = []\n",
    "#         # Ensure exactly 10 players in order\n",
    "#         player_keys = sorted(players.keys(), key=int)[:10]\n",
    "#         for player_key in player_keys:\n",
    "#             player_data = players[player_key]\n",
    "#             means.append(player_data[\"mean\"])\n",
    "#             covariances.append(player_data[\"covariance\"])\n",
    "#         if means and covariances:  # Ensure lists are not empty\n",
    "#             # Step 1: Calculate the average mean vector\n",
    "#             mean_total = np.mean(means, axis=0)\n",
    "#             # Step 2: Calculate the average covariance matrix\n",
    "#             covariance_total = np.mean(covariances, axis=0)\n",
    "#             # Step 3: Flatten the covariance matrix and combine it with the mean\n",
    "#             flattened_covariance = covariance_total.flatten()\n",
    "#             # Combined feature vector\n",
    "#             feature_vector = np.concatenate([mean_total, flattened_covariance])\n",
    "#         else:\n",
    "#             # Handle cases where means or covariances are empty\n",
    "#             feature_vector = np.array([])  # Empty feature vector\n",
    "#         # Store team vector\n",
    "#         team_feature_vectors[dataset_key][team_key] = feature_vector\n",
    "\n",
    "# for dataset, teams in team_feature_vectors.items():\n",
    "#     for team, vector in teams.items():\n",
    "#         if (dataset, team) in ground_truth:\n",
    "#             X.append(vector)\n",
    "#             y.append(ground_truth[(dataset, team)])\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "\n",
    "# NORMAL!!\n",
    "\n",
    "\n",
    "X = []  # Feature matrix\n",
    "y = []  # Labels\n",
    "# Extract features and labels for each team\n",
    "for (dataset_key, team_key), label in ground_truth_balanced.items():\n",
    "    team_data = data.get(dataset_key, {}).get(team_key)\n",
    "    if not team_data:\n",
    "        print(f\"Missing data for team {team_key} in dataset {dataset_key}. Skipping...\")\n",
    "        continue  # Skip if team data is missing\n",
    "    # Initialize list for storing individual player feature vectors\n",
    "    team_player_vectors = []\n",
    "    # Process each player in the team\n",
    "    for player_data in team_data.values():\n",
    "        mean_x, mean_y = player_data[\"mean\"]\n",
    "        cov_xx, cov_xy = player_data[\"covariance\"][0]\n",
    "        _, cov_yy = player_data[\"covariance\"][1] # because covariance is symmetric\n",
    "        # Create a player vector with mean and covariance components\n",
    "        player_vector = [mean_x, mean_y, cov_xx, cov_xy, cov_yy]\n",
    "        team_player_vectors.extend(player_vector)  # Add to team vector\n",
    "    # Append the team feature vector and its label\n",
    "    X.append(team_player_vectors)\n",
    "    y.append(label)\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Shape of feature matrix X:\", X.shape)\n",
    "print(\"Shape of label array y:\", y.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming X is your data matrix (57 samples x 50 features)\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Apply PCA\n",
    "# pca = PCA(n_components=20)  # You can choose the number of components you'd like (2 for visualization)\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# # Explained variance ratio\n",
    "# print(\"Explained variance by each component:\", pca.explained_variance_ratio_)\n",
    "# print(\"Total explained variance:\", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# # Map numeric labels to their names\n",
    "# label_names = {442: \"Label 442\", 4231: \"Label 4231\", 352: \"Label 352\", 0: \"Label 0\"}\n",
    "# label_colors = [label_names[label] for label in y]\n",
    "\n",
    "# # Define a colormap with distinct colors\n",
    "# cmap = plt.cm.get_cmap(\"tab10\", len(label_names))  # Use 'tab10' for distinct colors\n",
    "# colors = [cmap(i) for i in range(len(label_names))]\n",
    "\n",
    "# # Create a mapping of labels to colors\n",
    "# label_to_color = {label: colors[i] for i, label in enumerate(label_names.keys())}\n",
    "\n",
    "# # Visualize the transformed data (if you reduce to 2 components)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=[label_to_color[label] for label in y], label=label_colors)\n",
    "# plt.colorbar(scatter, label=\"Labels\")\n",
    "# plt.title('PCA of 50-Dimensional Data (2 Components)')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "\n",
    "# # Add legend for label names\n",
    "# handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=label_to_color[label], markersize=10) for label in label_names]\n",
    "# plt.legend(handles, label_names.values(), title=\"Labels\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [3, 5, 7, 9],\n",
    "#     'weights': ['uniform', 'distance'],\n",
    "#     'metric': ['euclidean', 'manhattan']\n",
    "# }\n",
    "\n",
    "# knn_cv = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_cv.fit(X_train, y_train)\n",
    "# best_knn = knn_cv.best_estimator_\n",
    "# print(f\"Best parameters: {knn_cv.best_params_}\")\n",
    "# print(f\"Best cross-validation score: {knn_cv.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = best_knn.predict(X_test)\n",
    "# test_accuracy = best_knn.score(X_test, y_test)\n",
    "# print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ==== Use Stratified K-Fold ====\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# ==== Define models with pipelines and parameter grids ====\n",
    "models_and_parameters = {\n",
    "    'RandomForest': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('clf', RandomForestClassifier(random_state=42)) #class balanced\n",
    "        ]),\n",
    "        'params': {\n",
    "            'clf__n_estimators': [10, 20, 30, 40, 50, 100],\n",
    "            'clf__max_depth': [None, 5, 10, 15, 20, 30, 50]\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'pipeline': Pipeline([\n",
    "          ('scaler', StandardScaler()),      \n",
    "            ('clf', SVC())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'clf__C': [0.1, 1],\n",
    "            'clf__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'pipeline': Pipeline([\n",
    "           ('scaler', StandardScaler()),      \n",
    "            ('clf', LogisticRegression(max_iter=1000))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'clf__C': [0.1, 1],\n",
    "            'clf__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'pipeline': Pipeline([\n",
    "            \n",
    "           ('scaler', StandardScaler()),      \n",
    "            ('clf', KNeighborsClassifier())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'clf__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "            'clf__metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "            'clf__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'clf__leaf_size': [10, 20, 30, 40, 50],\n",
    "            'clf__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==== Run GridSearchCV and evaluate with StratifiedKFold ====\n",
    "results = {}\n",
    "\n",
    "for name, mp in models_and_parameters.items():\n",
    "    print(f\"\\nüîç Running GridSearchCV for {name}...\")\n",
    "    \n",
    "    # Grid Search with StratifiedKFold\n",
    "    grid = GridSearchCV(mp['pipeline'], mp['params'], cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    results[name] = {\n",
    "        'best_score': grid.best_score_,\n",
    "        'best_params': grid.best_params_,\n",
    "        'best_estimator': best_model\n",
    "    }\n",
    "    \n",
    "    # Predict using cross_val_predict for fair evaluation\n",
    "    y_pred = cross_val_predict(grid.best_estimator_, X, y, cv=cv, method='predict')\n",
    "\n",
    "    \n",
    "\n",
    "    nested_score = cross_val_score(grid, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    print(f\"Nested CV Accuracy: {nested_score.mean():.4f}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Confusion Matrix & Report\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(f\"\\n‚úÖ {name} Results\")\n",
    "    print(f\"Best Score (Mean CV Accuracy): {grid.best_score_:.4f}\")\n",
    "    print(f\"Best Params: {grid.best_params_}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    # Plot Confusion Matrix with true labels\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'{name} - Confusion Matrix (Stratified K-Fold)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris  # Example dataset, replace with your own\n",
    "\n",
    "# ==== Load your dataset here ====\n",
    "# Replace this with your actual X and y\n",
    "\n",
    "\n",
    "# ==== Use Leave-One-Out Cross-Validation (LOO-CV) ====\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "# ==== Define models with pipelines and parameter grids ====\n",
    "models_and_parameters = {\n",
    "    'RandomForest': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('clf', RandomForestClassifier(random_state=42, class_weight='balanced')) #class balanced\n",
    "        ]),\n",
    "        'params': {\n",
    "            'clf__n_estimators': [10, 20, 30, 40, 50, 100],\n",
    "            'clf__max_depth': [None, 5, 10, 15, 20, 30, 50]\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'pipeline': Pipeline([\n",
    "          ('scaler', StandardScaler()),      \n",
    "            ('clf', SVC())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'clf__C': [0.1, 1],\n",
    "            'clf__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'pipeline': Pipeline([\n",
    "           ('scaler', StandardScaler()),      \n",
    "            ('clf', LogisticRegression(max_iter=1000))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'clf__C': [0.1, 1],\n",
    "            'clf__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'pipeline': Pipeline([\n",
    "           ('scaler', StandardScaler()),      \n",
    "            ('clf', KNeighborsClassifier())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'clf__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "            'clf__metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "            'clf__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'clf__leaf_size': [10, 20, 30, 40, 50],\n",
    "            'clf__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==== Run GridSearchCV with Leave-One-Out Cross-Validation ====\n",
    "results = {}\n",
    "\n",
    "for name, mp in models_and_parameters.items():\n",
    "    print(f\"\\nüîç Running GridSearchCV for {name}...\")\n",
    "    \n",
    "    # Grid Search with Leave-One-Out Cross-Validation\n",
    "    grid = GridSearchCV(mp['pipeline'], mp['params'], cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    results[name] = {\n",
    "        'best_score': grid.best_score_,\n",
    "        'best_params': grid.best_params_,\n",
    "        'best_estimator': best_model\n",
    "    }\n",
    "    \n",
    "    # Predict using cross_val_predict for fair evaluation\n",
    "    y_pred = cross_val_predict(best_model, X, y, cv=cv)\n",
    "    \n",
    "    # Confusion Matrix & Report\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(f\"\\n‚úÖ {name} Results\")\n",
    "    print(f\"Best Score (Mean CV Accuracy): {grid.best_score_:.4f}\")\n",
    "    print(f\"Best Params: {grid.best_params_}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    # Plot Confusion Matrix with true labels\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'{name} - Confusion Matrix (Leave-One-Out CV)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def plot_means_with_covariances(data, file_key, cluster_id):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    frames = data[file_key][cluster_id]\n",
    "\n",
    "    for frame_idx, stats in frames.items():\n",
    "        mean = np.array(stats['mean'])\n",
    "        cov = np.array(stats['covariance'])\n",
    "\n",
    "        # Plot mean\n",
    "        ax.scatter(*mean, label=f\"Frame {frame_idx}\")\n",
    "\n",
    "        # Plot covariance as ellipse\n",
    "        lambda_, v = np.linalg.eig(cov)\n",
    "        lambda_ = np.sqrt(lambda_)\n",
    "        angle = np.degrees(np.arctan2(*v[:, 0][::-1]))\n",
    "        ell = Ellipse(xy=mean,\n",
    "                      width=lambda_[0]*2, height=lambda_[1]*2,\n",
    "                      angle=angle, edgecolor='r', facecolor='none', lw=1)\n",
    "        ax.add_patch(ell)\n",
    "\n",
    "    ax.set_title(f\"Cluster: {cluster_id} in File: {file_key}\")\n",
    "    ax.set_xlabel(\"X Position\")\n",
    "    ax.set_ylabel(\"Y Position\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_with_covariances(\n",
    "    data=data,\n",
    "    file_key=\"./data/DFL_04_03_positions_raw_observed_DFL-COM-000002_DFL-MAT-J03WQQ.xml\",\n",
    "    cluster_id=\"DFL-CLU-00000P\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
